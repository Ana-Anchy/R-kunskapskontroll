# ============================
# Kunskapskontroll - Volvo Bilanalys fr√•n Blocket
# ============================


# ============================
# üì¶ Paket och bibliotek
# ============================

packages <- c("tidyverse", "readxl", "GGally", "glmnet", "broom", "caret", "rmarkdown", "scales")

installed <- packages %in% installed.packages()
if (any(!installed)) {
  install.packages(packages[!installed])
}

# ============================
# Ladda packages
# ============================

library(readr)
library(tibble)
library(dplyr)
library(ggplot2)
library(GGally)
library(glmnet)
library(broom)
library(caret)
library(rmarkdown)
library(scales)


# ============================
#  1. L√§s in och f√∂rbered data
# ============================

#L√§s in datan fr√•n en CSV-fil.
df <- read_csv2("C:/Users/anaba/OneDrive/Desktop/R/data_blocket_volvo.csv")

#Spara originaldatan som referens.
df_raw <- df  

#Konvertera viktiga kolumner (miltal, h√§stkrafter, pris, √•rsmodell) till numeriskt format
df$miles      <- as.numeric(df$miles)
df$hpower     <- as.numeric(df$hpower)
df$price      <- as.numeric(df$price)
df$year_model <- as.numeric(df$year_model)

numeric_cols <- c("price", "miles", "hpower", "year_model")
df[numeric_cols] <- lapply(df[numeric_cols], function(x) as.numeric(gsub(" ", "", x)))

#Identifiera rader som misslyckas vid konverteringen till numeriskt format
bad_rows <- df %>%
filter(is.na(price) | is.na(miles) | is.na(hpower) | is.na(year_model))

#Spara och visa problematiska rader i en separat CSV
bad_rows <- df %>% filter(if_any(all_of(numeric_cols), is.na))
write_csv(bad_rows, "problemrader.csv")

#visa bad rows
View(bad_rows)
write_csv(bad_rows, "problemrader.csv")

#antal bad rows
nrow(bad_rows)

#Rensa bort rader med saknade (NA) v√§rden i centrala numeriska variabler
df <- df %>% filter(!is.na(price), !is.na(miles), !is.na(hpower), !is.na(year_model))

#Visa en sammanfattning och kontrollera antalet NA-v√§rden
summary(df)
colSums(is.na(df))

#Rensa bort rader med saknade v√§rden
df <- df %>% filter(if_all(all_of(numeric_cols), ~ !is.na(.x)))

#Skapa nya variabler
df <- df %>% 
  mutate(
    age = 2024 - year_model,
    hk_div_age = hpower / (age + 1)
  )

# ============================
# 2. Boxplot & stapeldiagram f√∂r kategoriska variabler
# ============================

print(ggplot(df, aes(x = gear, y = price)) +
        geom_boxplot(fill = "skyblue") +
        labs(title = "Price by Gear Type", x = "Gear", y = "Price (SEK)") +
        theme_minimal())

# Load stringr
library(stringr)

# (e.g. "volvo" -> "Volvo")
df$brand <- str_to_title(df$brand)

# bar chart 
print(ggplot(df, aes(x = brand)) +
        geom_bar(fill = "coral") +
        labs(title = "Number of Cars per Brand", x = "Brand", y = "Count") +
        theme_minimal())


summary(model_num)
summary(model_cat)

# ============================
# 3. Exploratory Data Analysis (EDA)
# ============================

# √•ldervariabel (baserat p√• max √•r i datan)
df$age <- max(df$year_model, na.rm = TRUE) - df$year_model

# Transformation: H√§stkrafter / ‚àö(√•lder + 1)
df$hk_div_age <- df$hpower / sqrt(df$age + 1)

library(ggplot2)
library(scales)

#Prisf√∂rdelning per br√§nsletyp
ggplot(df, aes(x = fuel, y = price)) +
  geom_boxplot(fill = "lightblue", outlier.color = "red") +
  labs(
    title = "Prisf√∂rdelning per br√§nsletyp",
    x = "Br√§nsle",
    y = "Pris (kr)"
  ) +
  theme_minimal()

#Standardisera karosstyp 
df$type <- stringr::str_to_title(tolower(df$type))

#Prisvariation beroende p√• karosstyp
ggplot(df, aes(x = type, y = price)) +
  geom_boxplot(fill = "yellow", color = "black") +
  labs(
    title = "Prisf√∂rdelning per karosstyp",
    x = "Karosstyp",
    y = "Pris"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # rotera x-etiketter

#Visualisera f√∂rdelningen av miltal med ett histogram
ggplot(df, aes(x = miles)) +
  geom_histogram(binwidth = 5000, fill = "darkgreen", color = "black") +
  labs(title = "Distribution of Mileage", x = "Mileage (miles)", y = "Count") +
  scale_x_continuous(labels = comma) +
  theme_minimal()


#Visualisera f√∂rdelningen av h√§stkrafter (hp) med ett histogram
print(ggplot(df, aes(x = hpower)) +
        geom_histogram(binwidth = 10, fill = "purple", color = "white") +
        labs(title = "Distribution of Horsepower", x = "Horsepower", y = "Count") +
        theme_minimal())

#Visualisera f√∂rdelningen av priser med ett histogram.
print(ggplot(df, aes(x = price)) +
        geom_histogram(binwidth = 10000, fill = "pink", color = "white") +
        labs(title = "Distribution of Price", x = "Price (SEK)", y = "Count") +
        theme_minimal())



library(GGally)
library(scales)

#Anpassad funktion f√∂r att formatera y-axlar

# ============================
# 4. Correlation Matrix
# ============================

custom_axis <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) +
    geom_point(...) +
    scale_y_continuous(labels = comma) +
    scale_x_continuous(labels = comma)
}

custom_density <- function(data, mapping, ...) {
  ggplot(data = data, mapping = mapping) +
    stat_density(aes(y = ..density..), geom = "line", position = "identity", ...) +
    scale_y_continuous(labels = comma) +
    scale_x_continuous(labels = comma)
}


ggpairs(df %>% select(price, miles, hpower),
        title = "Correlation Matrix",
        upper = list(continuous = wrap("cor", size = 4)),
        lower = list(continuous = custom_axis),
        diag = list(continuous = custom_density))


# ============================
# 5. Train/Test Split med Caret
# ============================

index <- createDataPartition(df$price, p = 0.7, list = FALSE)
train_df <- df[index, ]
test_df <- df[-index, ]


# ============================
# . Alternative models (numeric and categorical)
# ============================

# Check how many levels each factor has
sapply(train_df[, c("gear", "brand", "model")], function(x) length(unique(x)))

#Skapa en numerisk regressionsmodell baserat p√• miltal och h√§stkrafter
model_num <- lm(price ~ miles + hpower, data = train_df)

#Omvandla v√§xell√•da, bilm√§rke och modell till faktorer f√∂r kategorisk analys

train_df <- train_df %>%
  group_by(gear) %>% filter(n() > 1) %>%
  group_by(brand) %>% filter(n() > 1) %>%
  group_by(model) %>% filter(n() > 1) %>%
  ungroup()

train_df$gear <- as.factor(train_df$gear)
train_df$brand <- as.factor(train_df$brand)
train_df$model <- as.factor(train_df$model)

if (
  length(unique(train_df$brand)) > 1 &
  length(unique(train_df$gear)) > 1 &
  length(unique(train_df$model)) > 1
) {
  model_cat <- lm(price ~ gear + brand + model, data = train_df)
  summary(model_cat)
} else {
  print("Skipping categorical model: one or more factors have only one level.")
}



summary(model_num)
summary(model_cat)


# ============================
# 6. OLS Regression p√• hela datan
# ============================


ols_model <- lm(price ~ miles + hpower, data = df)
summary(ols_model)

ols_preds <- predict(ols_model)
ols_r2 <- summary(ols_model)$r.squared
ols_rmse <- sqrt(mean((df$price - ols_preds)^2))
ols_bic <- BIC(ols_model)

cat("OLS R¬≤: ", round(ols_r2, 3), "\n")
cat("OLS RMSE: ", round(ols_rmse), "\n")
cat("OLS BIC: ", round(ols_bic), "\n")


# ============================
# 6.1 Enkel modell med transformerad variabel
# ============================

df <- df %>%
  mutate(
    age = 2024 - year_model + 1,
    age = ifelse(age <= 0 | is.na(age), NA, age),   # mark invalid ages
    hk_div_age = hpower / age
  )


simple_model <- lm(price ~ hk_div_age, data = df)
summary(simple_model)

ggplot(df, aes(x = hk_div_age, y = price)) +
  geom_point(alpha = 0.6, color = "steelblue") +
  geom_smooth(method = "lm", color = "darkred", se = FALSE) +
  labs(
    title = "Pris som funktion av h√§stkrafter / ‚àö(√•lder + 1)",
    x = "H√§stkrafter / ‚àö(√•lder + 1)",
    y = "Pris (SEK)"
  ) +
  theme_minimal()

# ================================
# 6.3 OLS Regression p√• train/test
# ================================

ols_model_test <- lm(price ~ miles + hpower, data = train_df)
summary(ols_model_test)

ols_preds <- predict(ols_model, newdata = test_df)
ols_r2 <- 1 - sum((test_df$price - ols_preds)^2) / sum((test_df$price - mean(test_df$price))^2)
ols_rmse <- sqrt(mean((test_df$price - ols_preds)^2))
ols_bic <- BIC(ols_model)

cat("Test R¬≤: ", round(ols_r2, 3), "\n")
cat("Test RMSE: ", round(ols_rmse), "\n")
cat("OLS BIC: ", round(ols_bic), "\n")


# ============================
# 7. Lasso Regression
# ============================

x <- as.matrix(df[, c("miles", "hpower")])
y <- df$price

lasso_cv <- cv.glmnet(x, y, alpha = 1)
best_lambda <- lasso_cv$lambda.min

lasso_model <- glmnet(x, y, alpha = 1, lambda = best_lambda)
lasso_preds <- predict(lasso_model, s = best_lambda, newx = x)

lasso_r2 <- 1 - sum((y - lasso_preds)^2) / sum((y - mean(y))^2)
lasso_rmse <- sqrt(mean((y - lasso_preds)^2))

cat("Lasso R¬≤: ", round(lasso_r2, 3), "\n")
cat("Lasso RMSE: ", round(lasso_rmse), "\n")
cat("Valt lambda: ", round(best_lambda, 2), "\n")

print(coef(lasso_model, s = best_lambda))

# ============================
# 8. Konfidens- och Prediktionsintervall
# ============================

new_car <- data.frame(miles = 15000, hpower = 140)

conf_interval <- predict(ols_model, newdata = new_car, interval = "confidence")
pred_interval <- predict(ols_model, newdata = new_car, interval = "prediction")

cat("\nKonfidensintervall:\n")
print(conf_interval)

cat("\nPrediktionsintervall:\n")
print(pred_interval)


# ============================
# 9. Exportera resultat
# ============================

results <- data.frame(
  Model = c("OLS", "Lasso"),
  R2 = c(ols_r2, lasso_r2),
  RMSE = c(ols_rmse, lasso_rmse),
  BIC = c(ols_bic, NA)
)

write.csv(results, "model_comparison.csv", row.names = FALSE)



# ============================
# 10. Extra: Cross-validation utv√§rdering (OLS)
# ============================
library(caret) 

train_control <- trainControl(method = "cv", number = 5)

cv_model <- train(price ~ miles + hpower,
                  data = df,
                  method = "lm",
                  trControl = train_control)

cat("\nCV RMSE (OLS): ", cv_model$results$RMSE, "\n")
cat("CV R¬≤ (OLS): ", cv_model$results$Rsquared, "\n")


# =====================================================
# üìä # üîö Slutsats och Resultat
# =====================================================

# - EDA genomf√∂rdes med hj√§lp av histogram, boxplots och korrelationsanalys.
# - J√§mf√∂relse mellan OLS och Lasso-regression visade tydliga m√∂nster.
# - Modellvalidering utf√∂rdes med b√•de train/test-split och 5-fold cross-validation.
# - Resultat exporterades till CSV f√∂r vidare presentation.
# - Konfidens- och prediktionsintervall ber√§knades f√∂r en ny bil.
# - Variabeltolkning visade att k√∂rstr√§cka och h√§stkrafter var mest avg√∂rande f√∂r f√∂rs√§ljningspriset.

# -----------------------------------------------------
# ‚ùì Fr√•gest√§llning 1: Vilka variabler har st√∂rst p√•verkan p√• priset f√∂r en begagnad bil?
# -----------------------------------------------------
# - B√•de OLS och Lasso identifierade:
#     ‚ñ∏ K√∂rstr√§cka (miles) ‚Äì negativ korrelation med priset
#     ‚ñ∏ H√§stkrafter (hpower) ‚Äì positiv korrelation med priset
# - Dessa variabler var statistiskt signifikanta och √•terkom i b√•da modellerna.

# -----------------------------------------------------
# ‚ùì Fr√•gest√§llning 2: Hur v√§l kan en regressionsmodell f√∂rklara variationen i priset?
# -----------------------------------------------------
# - OLS-modellen:
#     ‚ñ∏ R¬≤ ‚âà 0.693
#     ‚ñ∏ RMSE ‚âà 917.53kr

# - Lasso-modellen:
#     ‚ñ∏ R¬≤ ‚âà 0.680
#     ‚ñ∏ RMSE ‚âà 93 699 kr
#     ‚ñ∏ Lambda ‚âà 15 685

# - Cross-validation (OLS):
#     ‚ñ∏ CV R¬≤ ‚âà 0.684
#     ‚ñ∏ CV RMSE ‚âà 102.226 kr

# -----------------------------------------------------
# üß† Slutsats:
# -----------------------------------------------------
# B√•de OLS och Lasso visade god f√∂rklaringsgrad trots att endast tv√• variabler anv√§ndes.
# Lasso gav en n√•got l√§gre f√∂rklaringsgrad men kan bidra till en enklare modell och minska risken f√∂r √∂veranpassning.
# Modellerna lyckades f√∂rklara cirka 66‚Äì68 % av variationen i pris, vilket √§r rimligt med tanke p√• att viktiga faktorer som skick, utrustning och s√§song inte fanns med i datan.
# Cross-validation visade att modellen presterar stabilt √§ven p√• ny data, √§ven om felmarginalen (RMSE) √∂kar n√•got.
# Sammantaget visar analysen att regressionsmodeller kan ge v√§rdefulla insikter f√∂r priss√§ttning av begagnade bilar och hj√§lpa k√∂pare och s√§ljare att g√∂ra mer informerade beslut.

